<!DOCTYPE html>
<html lang="en" data-dark="false">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-MK50YGPJPH"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-MK50YGPJPH');
</script>

  <!--
  put your search engine verification (e.g. Google Search Console) tag here
-->

  


























<meta name="viewport" content="width=device-width, initial-scale=1">

<title>Projects | GeoHAI</title>

<link rel="icon" href="/preview/pr-47/images/icon.png">

<meta name="title" content="Projects">
<meta name="description" content="Geospatial Human-Centered Artificial Intelligence Research Lab. Advancing geospatial use-inspired research at the intersection of AI and human-centered design">

<meta property="og:title" content="Projects">
<meta property="og:site_title" content="GeoHAI">
<meta property="og:description" content="Geospatial Human-Centered Artificial Intelligence Research Lab. Advancing geospatial use-inspired research at the intersection of AI and human-centered design">
<meta property="og:url" content="">
<meta property="og:image" content="/preview/pr-47/images/share.jpg">
<meta property="og:locale" content="en_US">

<meta property="twitter:title" content="Projects">
<meta property="twitter:description" content="Geospatial Human-Centered Artificial Intelligence Research Lab. Advancing geospatial use-inspired research at the intersection of AI and human-centered design">
<meta property="twitter:url" content="">
<meta property="twitter:card" content="summary_large_image">
<meta property="twitter:image" content="/preview/pr-47/images/share.jpg">


  <meta property="og:type" content="website">


<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "WebSite",
    
    "name": "Projects",
    "description": "Geospatial Human-Centered Artificial Intelligence Research Lab. Advancing geospatial use-inspired research at the intersection of AI and human-centered design",
    "headline": "Projects",
    "publisher": {
      "@type": "Organization",
      "logo": { "@type": "ImageObject", "url": "/preview/pr-47/images/icon.png" }
    },
    "url": ""
  }
</script>

<link rel="alternate" type="application/rss+xml" href="/preview/pr-47/feed.xml">

  <!-- Google Fonts -->
<!-- automatically get url from fonts used in theme file -->

<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?display=swap&&family=Barlow:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600&amp;family=Roboto+Mono:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600" rel="stylesheet">

<!-- Font Awesome icons (load asynchronously due to size) -->

<link href="https://use.fontawesome.com/releases/v6.7.1/css/all.css" rel="preload" as="style" onload="this.onload = null; this.rel = 'stylesheet';">
<noscript>
  <link href="https://use.fontawesome.com/releases/v6.7.1/css/all.css" rel="stylesheet">
</noscript>

  <!-- third party styles -->
<!-- https://stylishthemes.github.io/Syntax-Themes/pygments/ -->
<link href="https://cdn.jsdelivr.net/gh/StylishThemes/Syntax-Themes/pygments/css-github/pygments-tomorrow-night-eighties.css" rel="stylesheet">

<!-- include all sass in styles folder -->


  
    <link href="/preview/pr-47/_styles/-theme.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/alert.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/all.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/anchor.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/background.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/body.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/bold.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/button.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/card.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/checkbox.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/citation.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/code.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/cols.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/dark-toggle.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/feature.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/figure.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/float.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/font.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/footer.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/form.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/grid.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/header.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/heading.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/highlight.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/icon.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/image.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/link.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/list.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/main.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/paragraph.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/portrait.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/post-excerpt.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/post-info.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/post-nav.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/project-excerpt.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/project-info.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/project-nav.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/project-portrait.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/quote.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/rectangular-grid.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/rule.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/search-box.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/search-info.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/section.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/table.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/tags.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/textbox.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/tooltip.css" rel="stylesheet">
  

  
    <link href="/preview/pr-47/_styles/util.css" rel="stylesheet">
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  


<!-- include all css in styles folder -->



  <!-- third party scripts -->
<script src="https://unpkg.com/@popperjs/core@2" defer></script>
<script src="https://unpkg.com/tippy.js@6" defer></script>
<script src="https://unpkg.com/mark.js@8" defer></script>

<!-- include all js in scripts folder -->


  <script src="/preview/pr-47/_scripts/anchors.js"></script>

  <script src="/preview/pr-47/_scripts/dark-mode.js"></script>

  <script src="/preview/pr-47/_scripts/fetch-tags.js"></script>

  <script src="/preview/pr-47/_scripts/search.js"></script>

  <script src="/preview/pr-47/_scripts/site-search.js"></script>

  <script src="/preview/pr-47/_scripts/tooltip.js"></script>


</head>

  <body>
    









<header class="background" style="background-color: #051B2B;" data-dark="true">
  <a href="/preview/pr-47/" class="home">
    
      <span class="logo logo-header">
        <img src="/preview/pr-47/images/icon.png" alt="logo">
        <img class="logo-name-1" src="/preview/pr-47/images/logo-name-1.png" alt="GEOHAI">
        <img class="logo-name-2" src="/preview/pr-47/images/logo-name-2.png" alt="Research Lab">
      </span>
    
    
      <span class="title" data-tooltip="Home">
        
        
      </span>
    
  </a>

  <input class="nav-toggle" type="checkbox" aria-label="show/hide nav">

  <nav>
    
    
      
        <a href="/preview/pr-47/projects/" data-tooltip="Project descriptions">
          Projects
        </a>
      
    
      
        <a href="/preview/pr-47/research/" data-tooltip="Published works">
          Publications
        </a>
      
    
      
        <a href="/preview/pr-47/products/" data-tooltip="Software, datasets, and more">
          Products
        </a>
      
    
      
        <a href="/preview/pr-47/blog/" data-tooltip="Musings and miscellany">
          Blog
        </a>
      
    
      
        <a href="/preview/pr-47/team/" data-tooltip="About our team">
          Team
        </a>
      
    
      
        <a href="/preview/pr-47/contact/" data-tooltip="Email, address, and location">
          Contact
        </a>
      
    
  </nav>
</header>

    <main>
      <!--
  modify main content of page:
  - add section breaks
  - attach section properties
  - wrap each table in div to allow for scrolling
  - filter out blank sections
-->








  
  
  

  <section class="background" data-size="page">
    <h1 id="projects">
<i class="icon fa-solid fa-project-diagram"></i>Projects</h1>

<p>We thank our sponsors, including but not limited to <a href="https://www.nsf.gov/">National Science Foundation</a>, <a href="https://www.nasa.gov/">NASA</a>, <a href="https://www.nrel.gov/">NREL</a>, <a href="https://www.nih.gov/">NIH</a> and <a href="https://popcouncil.org/">Population Council</a>. See each project for more details.</p>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<div class="search-box">
  <input type="text" class="search-input" oninput="onSearchInput(this)" placeholder="Search items on this page">
  <button disabled data-tooltip="Clear search" aria-label="clear search" onclick="onSearchClear()">
    <i class="icon fa-solid fa-magnifying-glass"></i>
  </button>
</div>

<!-- Added this to collect project tags -->

<div class="tags" data-link="/preview/pr-47/projects/">
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('action recognition')" data-tooltip='Show items with the tag "action recognition"'>
        action recognition
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('air pollution')" data-tooltip='Show items with the tag "air pollution"'>
        air pollution
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('covid-19')" data-tooltip='Show items with the tag "covid-19"'>
        covid-19
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('cryosphere')" data-tooltip='Show items with the tag "cryosphere"'>
        cryosphere
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('electrical grid')" data-tooltip='Show items with the tag "electrical grid"'>
        electrical grid
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('epidemiology')" data-tooltip='Show items with the tag "epidemiology"'>
        epidemiology
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('fire ecology')" data-tooltip='Show items with the tag "fire ecology"'>
        fire ecology
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('foundation model')" data-tooltip='Show items with the tag "foundation model"'>
        foundation model
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('geovisualization')" data-tooltip='Show items with the tag "geovisualization"'>
        geovisualization
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('graph neural networks')" data-tooltip='Show items with the tag "graph neural networks"'>
        graph neural networks
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('machine learning')" data-tooltip='Show items with the tag "machine learning"'>
        machine learning
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('public health')" data-tooltip='Show items with the tag "public health"'>
        public health
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('remote sensing')" data-tooltip='Show items with the tag "remote sensing"'>
        remote sensing
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('sea ice')" data-tooltip='Show items with the tag "sea ice"'>
        sea ice
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('spatiotemporal')" data-tooltip='Show items with the tag "spatiotemporal"'>
        spatiotemporal
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('uncertainty')" data-tooltip='Show items with the tag "uncertainty"'>
        uncertainty
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('video understanding')" data-tooltip='Show items with the tag "video understanding"'>
        video understanding
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('visual analytics')" data-tooltip='Show items with the tag "visual analytics"'>
        visual analytics
      </a>
    
  </div>

<div class="search-info"></div>

<div class="project-excerpt">
  <div class="project-content">
    
    
    <a href="/preview/pr-47/projects/estimating-air-pollution.html" class="project-title">Estimating Surface-Level Air Pollution</a>
    
    
      <div class="project-tags">
        


  <div class="tags" data-link="/preview/pr-47/projects">
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('spatiotemporal')" data-tooltip='Show items with the tag "spatiotemporal"'>
        spatiotemporal
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('machine learning')" data-tooltip='Show items with the tag "machine learning"'>
        machine learning
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('remote sensing')" data-tooltip='Show items with the tag "remote sensing"'>
        remote sensing
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('air pollution')" data-tooltip='Show items with the tag "air pollution"'>
        air pollution
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('public health')" data-tooltip='Show items with the tag "public health"'>
        public health
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('epidemiology')" data-tooltip='Show items with the tag "epidemiology"'>
        epidemiology
      </a>
    
  </div>


      </div>
      

    
    
    <p data-search="Ground stations characterize air quality by observing air pollution concentrations at the air pollution stations  On the other hand  people live and work in places relatively far from these sensors  So how do we quantify or study the impacts of air pollution on public health  Satellite observations can help with that  However  satellites derive measures of pollution in columns of air as seen from space   Air pollution  as experienced by humans  however  is on the surface  Therefore  the research challenge is to estimate surface level air pollution at any given point  on any given day  by relying on the satellite observations of columns of air  and limited ground based calibration data In this project  we are estimating daily surface level concentrations of PM2 5  Ozone  with NO2 in progress  in the U S  with 1KM resolution  to support our public health and epidemeologist collaborators as well as other potential users interested in air pollution datasets  We develop and use specialized deep learning methods for this purpose                              Spatial and temporal analysis of PM2 5 predictions during the 2020 California August Complex Fire  The columns represent  1  CONUS wide model predictions   2  Zoomed in predictions of the wildfire region  sharing the same colorbar as  1   ranging from 0 to 50 μg m3    3  HMS wildfire smoke density polygons  and  4  prediction bias errors  Rows correspond to four dates during different wildfire phases  ignition  18 August 2020   spreading  2 September 2020   intensification  18 September 2020   and later progression  2 October 2020   See the published paper            Deep Learning for Fusion of Ground  and Satellite based Measurements for Estimating Surface level Air Pollution ConcentrationSatellite observations and ground level measurements both play a role in air pollution modeling  however  they have their strengths and limitations  Remote sensing data measures the columnar concentration of pollutants from the atmosphere  providing indirect but valuable indicators for ground level air pollution  Remote sensing data usually provides broad spatial coverage  enabling large scale  e g   global and regional  modeling work  These observations are particularly valuable for regions lacking ground monitoring stations  such as rural and remote areas  Furthermore  the high spatiotemporal resolution of remote sensing products provides insights into the study of finer scale variations across space and time  For example  the MODIS AOD data for PM2 5 provide daily 1 km resolution observations  which is critical to capture inter urban patterns and daily variations  Similarly  Sentinel 5P from the TROPOMI sensor offers a daily but coarser spatial resolution  5 5km x 3 5km  for various air pollutants  including O3  NO2  and SO2  Despite those advantages  satellite data has its limitations  It provides columnar measurements  which pose a challenge for modeling work and require complex calibration to infer the ground level concentrations  Moreover  meteorological factors  e g   cloud cover and snow  and surface conditions  e g   bright surface  can affect remote observations and result in missing observations Ground based measurements are often considered  gold standard   providing highly accurate pollution data  Unlike remote sensing data  which measures columnar concentrations  ground based data measures pollutants directly at the surface level  omitting the necessity of calibration  Networks like the U S  EPA monitoring system provide high accuracy  ground level data for air pollutants across the U S   commonly used as ground truths in modeling  However  the primary drawback of ground level data from monitoring stations is its sparse spatial coverage  particularly in rural and remote areas  The uneven spatial distribution is another limitation  For example  there are many more monitoring stations in states like California than in its neighboring state  Nevada  which only has limited stations   The temporal resolution of ground level data has improved in recent years  and most stations now provide daily measurements  However  some stations still only provide measurements every three days  Another drawback is the challenge of timely measurements during emergent events  such as the increasing wildfire events  Deploying temporary monitoring stations in affected areas to measure sudden increases in air pollution is usually difficult but crucial for modeling extreme values  The emergence of low cost sensors  such as PurpleAir and Love My Air  LMA   address the problem to some degree  with a broader spatial coverage and distribution  However  the quality of the measurements for calibration of high resolution models poses additional research challanges when compared to the  gold standard  from EPA stations Machine learning models have become a dominant approach for fusing satellite and ground level data to estimate different air pollutants  Due to the capability to handle large datasets and capture complex non linear relationships  machine learning models provide more accurate and granular estimates than traditional statistical models  Especially  the advancements in hardware acceleration allows the machine learning model to harness more from the abundant remote sensing data  Common approaches for air pollution estimation can be divided into the following categories  tree based models  deep learning based models  and ensemble models Deep learning models leverage neural networks to capture complex spatiotemporal dependencies in high dimensional datasets  They can effectively fuse different data sources  such as in situ measurements  satellite images  and reanalysis data  Different types of neural networks are designed to approach the problem from various perspectives  See our publications and products to learn more on how we leverage deep learning to estimate surface level air pollution This project is a collaboration with National Jewish Health  James Crooks  and Elizabeth Regan  and is funded by NIH under grant  R21ES032973 and Colorado Population Center funded by Eunice Kennedy Shriver National Institute of Child Health  amp  Human Development of the National Institutes of Health  P2CHD066613  " class="project-excerpt-content">
      Ground-stations characterize air quality by observing air pollution concentrations at the air pollution stations. On the other hand, people live and work in places relatively far from these sensors. So how do we quantify or study the impacts of air pollution on public health? Satellite observations can help with that!...
      <a href="/preview/pr-47/projects/estimating-air-pollution.html" class="read-more">Continue reading.</a>
    </p>
  </div>

  
    <div class="project-image-container">
      <a href="/preview/pr-47/projects/estimating-air-pollution.html" aria-label="View Estimating Surface-Level Air Pollution">
        <img src="https://res.cloudinary.com/dz3zgmhnr/image/upload/v1750457206/2021-07-05_pred_geoclip_with_boundary_cropped-min-min_r0z2yv.png" alt="Estimating Surface-Level Air Pollution" class="project-image">
      </a>
    </div>
  
</div>

<div class="project-excerpt">
  <div class="project-content">
    
    
    <a href="/preview/pr-47/projects/forecasting-post-fire-recovery.html" class="project-title">Forecasting Post-Fire Vegetation Recovery using Deep Learning</a>
    
    
      <div class="project-tags">
        


  <div class="tags" data-link="/preview/pr-47/projects">
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('machine learning')" data-tooltip='Show items with the tag "machine learning"'>
        machine learning
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('remote sensing')" data-tooltip='Show items with the tag "remote sensing"'>
        remote sensing
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('spatiotemporal')" data-tooltip='Show items with the tag "spatiotemporal"'>
        spatiotemporal
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('fire ecology')" data-tooltip='Show items with the tag "fire ecology"'>
        fire ecology
      </a>
    
  </div>


      </div>
      

    
    
    <p data-search="In the aftermath of wildland fires  understanding and forecasting the trajectories and patterns of vegetation regrowth is crucial for sustainable land management and environmental conservation  The major question is whether and how an area  recovers  in the short and long term We train deep learning models to capture spatial and temporal dependencies in multi sensor remote sensing data  enabling the prediction of post fire landscape evolution at time intervals ranging from monthly to annually  We develop LSTM  and ConvLSTM based architectures which allows us to harness the spatial dependencies inherent in remote sensing data  producing high resolution spatio temporal forecasting models                 Time series of NDVI  Normalized Vegetation Index  before and after a fire in August 2011      By developing these forecasting models  our project aims to not only provide monthly predictions of post fire vegetation recovery but also generate recovery potential surfaces for recent fires  aiding land managers in targeted interventions  The convolutional aspect enhances the model s ability to discern spatial patterns  offering a comprehensive understanding of how different intra fire regions recover over time                 Example of Leaf Area Index  LAI  at a point burned by a forest fire  9 years before and after the fire disturbanse      This cutting edge fusion of deep learning and remote sensing technologies has potential to advance our insights into the complex dynamics of post fire landscape recovery  contributing valuable information for adaptive  restorative land management practices and ecological resilience in the face of increasing wildfires Click here to watch a brief video discussing preliminary work This project is funded by a National Science Foundation Graduate Research Fellowship under Grant No  DGE 2040434 " class="project-excerpt-content">
      In the aftermath of wildland fires, understanding and forecasting the trajectories and patterns of vegetation regrowth is crucial for sustainable land management and environmental conservation. The major question is whether and how an area ‘recovers’ in the short and long term.

      <a href="/preview/pr-47/projects/forecasting-post-fire-recovery.html" class="read-more">Continue reading.</a>
    </p>
  </div>

  
    <div class="project-image-container">
      <a href="/preview/pr-47/projects/forecasting-post-fire-recovery.html" aria-label="View Forecasting Post-Fire Vegetation Recovery using Deep Learning">
        <img src="https://res.cloudinary.com/dz3zgmhnr/image/upload/t_gen-fill-4-3/v1736116173/ee6b4452-a766-44d3-aea4-57389b583616.png" alt="Forecasting Post-Fire Vegetation Recovery using Deep Learning" class="project-image">
      </a>
    </div>
  
</div>

<div class="project-excerpt">
  <div class="project-content">
    
    
    <a href="/preview/pr-47/projects/foundation-model-multi-source.html" class="project-title">Foundation Models for Multi-Source Pre-Fusion and Few-shot Learning</a>
    
    
      <div class="project-tags">
        


  <div class="tags" data-link="/preview/pr-47/projects">
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('machine learning')" data-tooltip='Show items with the tag "machine learning"'>
        machine learning
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('remote sensing')" data-tooltip='Show items with the tag "remote sensing"'>
        remote sensing
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('foundation model')" data-tooltip='Show items with the tag "foundation model"'>
        foundation model
      </a>
    
  </div>


      </div>
      

    
    
    <p data-search="This project develops a foundation model that leverages multi source satellite data to enable few shot learning for various environmental monitoring tasks  We utilize both Synthetic Aperture Radar  SAR  data from Sentinel 1 and optical imagery from Sentinel 2  SAR  which emits and receives radar signals  captures surface texture and moisture content and can penetrate cloud cover  making it invaluable for several applications such as flood detection during cloudy conditions  In contrast  optical sensors capture light in visible and near infrared spectra  providing detailed information on vegetation  soil  and water bodies  crucial for land use classification and fire detection These data sources are complementary  SAR provides reliable data under all weather conditions  while optical sensors offer high resolution surface details in clear weather  Our project aims to train foundation models using these diverse data sources  enabling them to pre fuse the information  This pre fusion helps the model perform robustly in downstream tasks  even if only one data source is available  such as using SAR data alone for flood mapping during overcast conditions  or optical alone for land use classification                             Sentinel 1 SAR  left column  and Sentinel 2 Optical  middle and right columns  capture different information during the four seasons  the four rows   with the two sensors complementing each other s information on moisture  greenness    SAR primarily captures volume scattering  while optical relies on surface reflectivity  Images taken from the SSL4EO  dataset            Intellectual MeritThe intellectual challenge of this project lies in the development of a foundation model trained on multi source data that also supports source specific encoders  This approach not only enhances the model s generalizability across different environmental monitoring tasks but also improves performance in single source scenarios  By integrating diverse data types from the outset  the model learns to extract and combine the unique strengths of each data source  thereby improving its predictive capabilities and adaptability to various environmental phenomena Broader ImpactsThe ability to accurately monitor and predict environmental changes with minimal data samples has profound implications for disaster response  environmental protection  and sustainable development  This project s advancements in multi source data fusion and few shot learning will help ensure that critical decisions in emergency management and conservation can be made swiftly and based on reliable data  regardless of weather conditions  Ultimately  this technology could greatly enhance our ability to respond to natural disasters and manage natural resources more effectively This project is a GeoHAI Lab collaborative effort among students and postdocs in the lab  and funded by NASA and NSF " class="project-excerpt-content">
      This project develops a foundation model that leverages multi-source satellite data to enable few-shot learning for various environmental monitoring tasks. We utilize both Synthetic Aperture Radar (SAR) data from Sentinel-1 and optical imagery from Sentinel-2. SAR, which emits and receives radar signals, captures surface texture and moisture content and can...
      <a href="/preview/pr-47/projects/foundation-model-multi-source.html" class="read-more">Continue reading.</a>
    </p>
  </div>

  
    <div class="project-image-container">
      <a href="/preview/pr-47/projects/foundation-model-multi-source.html" aria-label="View Foundation Models for Multi-Source Pre-Fusion and Few-shot Learning">
        <img src="https://res.cloudinary.com/dz3zgmhnr/image/upload/t_fit-grey-4-3/v1735955434/eac27541-722d-4060-b1e1-899c7c073e5c.png" alt="Foundation Models for Multi-Source Pre-Fusion and Few-shot Learning" class="project-image">
      </a>
    </div>
  
</div>

<div class="project-excerpt">
  <div class="project-content">
    
    
    <a href="/preview/pr-47/projects/geographic-covid-forecast.html" class="project-title">Geographic Forecasting of COVID-19 Case and Hospitalizations Incidence</a>
    
    
      <div class="project-tags">
        


  <div class="tags" data-link="/preview/pr-47/projects">
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('spatiotemporal')" data-tooltip='Show items with the tag "spatiotemporal"'>
        spatiotemporal
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('machine learning')" data-tooltip='Show items with the tag "machine learning"'>
        machine learning
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('covid-19')" data-tooltip='Show items with the tag "covid-19"'>
        covid-19
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('public health')" data-tooltip='Show items with the tag "public health"'>
        public health
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('epidemiology')" data-tooltip='Show items with the tag "epidemiology"'>
        epidemiology
      </a>
    
  </div>


      </div>
      

    
    
    <p data-search="The COVID 19 pandemic s severe impact highlighted the need for accurate and timely forecasting of case and hospitalizations in each state and county to enable effective planning and resource allocation  The need for medical equipment across the country was so severe and timely that at times  states outbid each other to obtain such resources  Traveling nurses were sent around the country to help alleviate strain on local healthcare providers The US COVID 19 Forecast Hub  coordinated  standardized and created centralized infrastructure for scientists and epidemiologists around the country to submit case and hospitalization forecasts every week to help support operational planning  These forecasts were then evaluated  combined  and ensembled for reporting by the U S  Centers for Disease Control and Prevention  The methods standardized by the Forecast Hub has now become the standard way for forecasting flu  COVID 19  RSV  and even West Nile Virus forecasting in the US  and has inspired the European Forecast Hub During the pandemic  however  it became quickly clear how challenging the task of forecasting was  and how models were failing  Case forecasts were not reliable  and hospitalization and deaths forecasts  while more reliable  still left more to be desired  Most forecasting models struggled  especially during new variant surges  when they were most needed In collaboration with Population Council  we were one of the active teams throughout the pandemic to submit weekly forecasts to the Hub using machine learning by integrating spatiotemporal features  Our models  according to our own as well as independent evaluations  were relatively successful in capturing the trends within prediction intervals  especially during peaks  see our publications  linked below  We extended Long Short Term Memory  LSTM  networks for forecasting daily state level incident hospitalizations in the U S   as well as weekly case forecasts for U S  counties  Notably  we realized early on during the pandemic that county level and state level social media connectivity helped quantify the amount of spread from one county to another  or one state to another  We used social media connectivity to created proxies for inter state and inter county population interaction  capturing transmission dynamics across space and time  We also developed multi horizon ensembling strategy to balance between consistency and forecasting error Evaluations at multiple stages showed that our relatively simple models were effective and superior to most other models during the forecasting period  For instance  evaluating performance against the COVID 19 Forecast Hub ensemble model during the Omicron surge shows consistent superiority of our model  On average  our model surpasses the ensemble by 27  42  54  and 69 hospitalizations per state on the  7  th     14  th     21  st    and  28  th   forecast days  respectively  Data ablation experiments confirm the spatial features  predictive power  highlighting its effectiveness in enhancing forecasting models This research not only advances hospitalization forecasting but also underscores the significance of spatial spillover features refined using social media data  in improving predictive performance in modeling the complex dynamics of infectious disease spread " class="project-excerpt-content">
      The COVID-19 pandemic’s severe impact highlighted the need for accurate and timely forecasting of case and hospitalizations in each state and county to enable effective planning and resource allocation. The need for medical equipment across the country was so severe and timely that at times, states outbid each other to...
      <a href="/preview/pr-47/projects/geographic-covid-forecast.html" class="read-more">Continue reading.</a>
    </p>
  </div>

  
    <div class="project-image-container">
      <a href="/preview/pr-47/projects/geographic-covid-forecast.html" aria-label="View Geographic Forecasting of COVID-19 Case and Hospitalizations Incidence">
        <img src="https://res.cloudinary.com/dz3zgmhnr/image/upload/t_gen-fill-4-3/v1734975051/2020-COVID19_STGXB_hwjqcj.webp" alt="Geographic Forecasting of COVID-19 Case and Hospitalizations Incidence" class="project-image">
      </a>
    </div>
  
</div>

<div class="project-excerpt">
  <div class="project-content">
    
    
    <a href="/preview/pr-47/projects/network-vis-energy-grids.html" class="project-title">Network Geovisualization for Electricity Grid Distribution Systems</a>
    
    
      <div class="project-tags">
        


  <div class="tags" data-link="/preview/pr-47/projects">
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('geovisualization')" data-tooltip='Show items with the tag "geovisualization"'>
        geovisualization
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('visual analytics')" data-tooltip='Show items with the tag "visual analytics"'>
        visual analytics
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('electrical grid')" data-tooltip='Show items with the tag "electrical grid"'>
        electrical grid
      </a>
    
  </div>


      </div>
      

    
    
    <p data-search="Electrical grids are geographical and topological structures whose voltage states are challenging to represent efficiently for visual analysis especially at scale  The current common practice is to use colored contour maps  i e   heatmaps   yet these can misrepresent the data primarily because they average values over geographic distance  while ignoring network  i e   topological  distance  This especially becomes a problem when maps are  zoomed  out  where averaging will lead to the erasure of important larger or anomalous values                             A geographic heatmap  top left  erases the network structure  and also  leads to loss of values  as can be seen in bottom left   compared to showing the original values in a symbol glyph map  top right              which preserves the values on both ends of the data distribution  bottom right   However  glyp maps can t be used for larger scale visualizatoins  where the map is             essentially  zoomed out  for larger networks          This project develops novel network heatmap visualizations as well as  a web application  examining the suitability of four alternative visualization methods for depicting voltage data in a geographically dense distribution system Voronoi polygons  H3 tessellations  S2 tessellations  and a network weighted contour map  which this last one being a novel method developed by us  We find that Voronoi tessellations and network weighted contour maps more accurately represent the statistical distribution of the data than regular contour maps  but we intend to experiment with larger scale visualizations to see how network planners or managers can gain quick understanding of the network state using these methods This project is funded by the National Renewable Energy Laboratory in collaboration with the Visualization team led by Kristi Potter and Kenny Gruchalla " class="project-excerpt-content">
      Electrical grids are geographical and topological structures whose voltage states are challenging to represent efficiently for visual analysis, especially at scale. The current common practice is to use colored contour maps (i.e., heatmaps), yet these can misrepresent the data primarily because they average values over geographic distance, while ignoring network...
      <a href="/preview/pr-47/projects/network-vis-energy-grids.html" class="read-more">Continue reading.</a>
    </p>
  </div>

  
    <div class="project-image-container">
      <a href="/preview/pr-47/projects/network-vis-energy-grids.html" aria-label="View Network Geovisualization for Electricity Grid Distribution Systems">
        <img src="https://res.cloudinary.com/dz3zgmhnr/image/upload/t_crop-4-3/v1729718116/nrel-bus-voronoi_ksufcs.png" alt="Network Geovisualization for Electricity Grid Distribution Systems" class="project-image">
      </a>
    </div>
  
</div>

<div class="project-excerpt">
  <div class="project-content">
    
    
    <a href="/preview/pr-47/projects/sea-ice-charting.html" class="project-title">Sea Ice Mapping and Uncertainty Quantification</a>
    
    
      <div class="project-tags">
        


  <div class="tags" data-link="/preview/pr-47/projects">
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('spatiotemporal')" data-tooltip='Show items with the tag "spatiotemporal"'>
        spatiotemporal
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('machine learning')" data-tooltip='Show items with the tag "machine learning"'>
        machine learning
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('cryosphere')" data-tooltip='Show items with the tag "cryosphere"'>
        cryosphere
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('sea ice')" data-tooltip='Show items with the tag "sea ice"'>
        sea ice
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('uncertainty')" data-tooltip='Show items with the tag "uncertainty"'>
        uncertainty
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('remote sensing')" data-tooltip='Show items with the tag "remote sensing"'>
        remote sensing
      </a>
    
  </div>


      </div>
      

    
    
    <p data-search="        Sea ice charting is vital for navigating safely through icy waters and for scientists studying climate change in polar regions  Traditionally  experts at national ice centers manually interpret various types of satellite data to create sea ice maps using a standard called SIGRID 3  This manual process is slow and struggles to keep up with rapidly changing conditions in the Arctic  Our project aims to speed this process up while making it more accurate by using geospatial principles in developing  deep learning methods  using high  and low resolution satellite imagery     We primarily use data from Synthetic Aperture Radar  SAR  and Passive Microwave sensors  and some lidar altimetry in  related projects    SAR works by sending radar signals from satellites and capturing the reflections  providing images that can see through clouds and darkness  Passive Microwave sensors detect natural microwave signals emitted by the sea ice  useful for understanding ice concentration and type  These technologies are powerful but come with challenges different ice types can look similar in satellite images  causing confusion for both a human or algorithm  and the level of detail  spatial resolution  varies with the sensor type  affecting the accuracy of ice charts                                 Example of SAR imagery  a  d  and g   sea ice charts  c  f  and i   and sea ice probability maps  b  e  and h                     Intellectual Merit    Our project advances the field by integrating SAR with Passive Microwave data through deep learning  and developing training methods to fully utilize limited training samples available to scientists  This integration partially helps tackle the problem of signal ambiguity where different ice types may appear similar  We are also developing new ways to enhance model accuracy using specialized algorithms that consider the varying levels of detail captured by different sensors  as well as partial concentrations of sea ice in training labels  Additionally  we explore how transfer learning  a method where a model developed for one task is reused on another  can be adapted for SAR data to handle different ice conditions more effectively     An essential part of our work involves quantifying how certain the model is about its predictions  This uncertainty measurement lets sea ice chart analysts trust  correct  or verify the automated maps  ensuring they can be used confidently in real world navigation and research                                 Sea ice charting example  Top row  high resolution Sentinel 1 SAR image  2nd row  Low  resolution passive Microwave imagery  3rd row  manually generated ice charts  4th row  Deep learning based generated ice charts  along with uncertainty map to the right                  Broader Impacts    By automating sea ice charting  we make it possible for ships to navigate polar regions more safely and efficiently  potentially opening up new maritime routes  More reliable and timely maps also aid climate scientists in monitoring how quickly ice is forming or melting  which is crucial for understanding global climate change  This project not only demonstrates the potential of AI in environmental science but also showcases technological applications in better monitoring and studying our planet " class="project-excerpt-content">
      Sea ice charting is vital for navigating safely through icy waters and for scientists studying climate change in polar regions. Traditionally, experts at national ice centers manually interpret various types of satellite data to create sea ice maps using a standard called SIGRID-3. This manual process is slow and struggles...
      <a href="/preview/pr-47/projects/sea-ice-charting.html" class="read-more">Continue reading.</a>
    </p>
  </div>

  
    <div class="project-image-container">
      <a href="/preview/pr-47/projects/sea-ice-charting.html" aria-label="View Sea Ice Mapping and Uncertainty Quantification">
        <img src="https://res.cloudinary.com/dz3zgmhnr/image/upload/t_blue-fill-4-3/v1729813611/fig12_wdh7dy_c_pad_w_400_h_400_ultszf.png" alt="Sea Ice Mapping and Uncertainty Quantification" class="project-image">
      </a>
    </div>
  
</div>

<div class="project-excerpt">
  <div class="project-content">
    
    
    <a href="/preview/pr-47/projects/spatiotemporal-graph-action.html" class="project-title">Spatiotemporal Graph Neural Networks for Action Segmentation in Multimodal Data</a>
    
    
      <div class="project-tags">
        


  <div class="tags" data-link="/preview/pr-47/projects">
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('action recognition')" data-tooltip='Show items with the tag "action recognition"'>
        action recognition
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('graph neural networks')" data-tooltip='Show items with the tag "graph neural networks"'>
        graph neural networks
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('video understanding')" data-tooltip='Show items with the tag "video understanding"'>
        video understanding
      </a>
    
  </div>


      </div>
      

    
    
    <p data-search="Keystep recognition is a fine grained video understanding task that aims to classify small  heterogeneous steps within long form videos of human activities  Current approaches have poor performance  reaching 35 42  accuracy on the Ego Exo4D benchmark dataset In collaboration with Intel Labs  we developed a flexible graph learning framework for fine grained keystep recognition that achieves state of the art performance  Our approach  termed GLEVR  consists of constructing a graph where each video clip of the egocentric video corresponds to a node  We further leverage alignment between egocentric and exocentric videos during training for improved inference on egocentric videos  as well as adding automatic captioning as an additional modality  This simple  graph based approach is able to effectively learn long term dependencies in egocentric videos  Furthermore  the graphs are sparse and computationally efficient  substantially outperforming larger models We perform extensive experiments on the Ego Exo4D dataset and show that our proposed flexible graph based framework notably outperforms existing methods GLEVR won 1st place in the 2025 Ego Exo4D keystep recognition challenge  out of more than 20 team submissions  Check out the extended abstract on arxiv Check out the code repository                             Example sample from the  Ego Exo4D  benchmark  with multi modal ego  and exo centric videos  narrations  and commentaries                                  " class="project-excerpt-content">
      Keystep recognition is a fine-grained video understanding task that aims to classify small, heterogeneous steps within long-form videos of human activities. Current approaches have poor performance, reaching 35-42% accuracy on the Ego-Exo4D benchmark dataset.

      <a href="/preview/pr-47/projects/spatiotemporal-graph-action.html" class="read-more">Continue reading.</a>
    </p>
  </div>

  
    <div class="project-image-container">
      <a href="/preview/pr-47/projects/spatiotemporal-graph-action.html" aria-label="View Spatiotemporal Graph Neural Networks for Action Segmentation in Multimodal Data">
        <img src="https://res.cloudinary.com/dz3zgmhnr/image/upload/v1734927974/2a12e4dc-2fe6-4002-9c1c-1c8508e49638.png" alt="Spatiotemporal Graph Neural Networks for Action Segmentation in Multimodal Data" class="project-image">
      </a>
    </div>
  
</div>

<div class="project-excerpt">
  <div class="project-content">
    
    
    <a href="/preview/pr-47/projects/va-dengue.html" class="project-title">Visual Analytics for Spatiotemporal Analysis of Dengue Serotypes</a>
    
    
      <div class="project-tags">
        


  <div class="tags" data-link="/preview/pr-47/projects">
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('visual analytics')" data-tooltip='Show items with the tag "visual analytics"'>
        visual analytics
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('spatiotemporal')" data-tooltip='Show items with the tag "spatiotemporal"'>
        spatiotemporal
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('geovisualization')" data-tooltip='Show items with the tag "geovisualization"'>
        geovisualization
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('epidemiology')" data-tooltip='Show items with the tag "epidemiology"'>
        epidemiology
      </a>
    
  </div>


      </div>
      

    
    
    <p data-search="Dengue is a mosquito born virus  endemic to regions populated by over half the globe  Its four serotypes move and interact at global and regional scales  although these patterns are not well understood by researchers   GeoDEN aims to aid epidemiologists in this persuit  It is a visual analytics tool for investigating spatiotemporal patterns in dengue reports Static maps and animations limit the analytical depth and scope of visualization  and analysis of serotype movement and interaction has been limited to local scales  Therefore  researchers wanted a tool which enabled this sort of analysis  GeoDEN was designed with our target users throughout the design process  ensuring it fulfills these tasks GeoDEN uses a system of linked visualizations and filtering mechanisms   It enables analysis at various spatial and temporal scales  and the techniques in GeoDEN can be adapted for exploring other epidemiology and disease incident datasets " class="project-excerpt-content">
      Dengue is a mosquito-born virus, endemic to regions populated by over half the globe. Its four serotypes move and interact at global and regional scales, although these patterns are not well understood by researchers. GeoDEN aims to aid epidemiologists in this persuit. It is a visual analytics tool for investigating...
      <a href="/preview/pr-47/projects/va-dengue.html" class="read-more">Continue reading.</a>
    </p>
  </div>

  
    <div class="project-image-container">
      <a href="/preview/pr-47/projects/va-dengue.html" aria-label="View Visual Analytics for Spatiotemporal Analysis of Dengue Serotypes">
        <img src="https://res.cloudinary.com/dz3zgmhnr/image/upload/t_gen-fill-4-3/v1729526914/geoden_eexl8a.png" alt="Visual Analytics for Spatiotemporal Analysis of Dengue Serotypes" class="project-image">
      </a>
    </div>
  
</div>

<div class="project-excerpt">
  <div class="project-content">
    
    
    <a href="/preview/pr-47/projects/fusion-altimetry.html" class="project-title">Fusing Remote Sensing Imagery and Satellite-Based Laser Altimetry (ICESat-2)</a>
    
    
      <div class="project-tags">
        


  <div class="tags" data-link="/preview/pr-47/projects">
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('spatiotemporal')" data-tooltip='Show items with the tag "spatiotemporal"'>
        spatiotemporal
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('machine learning')" data-tooltip='Show items with the tag "machine learning"'>
        machine learning
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('cryosphere')" data-tooltip='Show items with the tag "cryosphere"'>
        cryosphere
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('sea ice')" data-tooltip='Show items with the tag "sea ice"'>
        sea ice
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('uncertainty')" data-tooltip='Show items with the tag "uncertainty"'>
        uncertainty
      </a>
    
      <a href="javascript:void(0);" class="tag" onclick="toggleTag('remote sensing')" data-tooltip='Show items with the tag "remote sensing"'>
        remote sensing
      </a>
    
  </div>


      </div>
      

    
    
    <p data-search="    This project integrates remote sensing and altimetry data to enhance sea ice mapping  with applications and methods generalizable to other application areas  including land remote sensing  We utilize Synthetic Aperture Radar  SAR  from Sentinel 1  which provides high resolution images capable of penetrating clouds and darkness  and optical satellite imagery from Sentinel 2  which captures visible and near infrared light to capture detailed ice surface characteristics  SAR measures volume scattering of active radar signals  whereas optical remote sensing measures surface reflectance and passively collected information  so they measure very different physical properties  However  both satellites face challenges  such as signal ambiguities in distinguishing ice types and limitations in spatial resolution and coverage under certain conditions  Additionally  while SAR can penetrate cloud cover  optical sensors like those on Sentinel 2 are hindered by it  yet they offer richer surface detail under clear conditions  Also  polar regions are dark 6 months at a time  so optical imagery is severely limited by lack of daylight                                 A comparison of ATL03 and ATL07 ICESat 2 products over Canadian Archipelago captured on 2022 01 05  Background image is Sentinel 1 HH polarization  ATL07 h coarse std has a 10km of coarse segment length                  Satellite altimetry  particularly from ICESat 2  offers precise measurements of ice surface heights using laser pulses  These measurements are crucial for measuring ice elevation freeboard and topography from space  ICESat 2 s lasers provide excellent precision  but as everything good in life goes  it has severe limitations  ICESat 2 has limited spatial coverage along linear track compared to the more extensive imaging swath of satellite imagery  such as those taken by Sentinel 1 or 2 satellites  Put differently  the altitudes are measured along a very narrow line  well  three narrow lines in this case   and not a grid  By the time ICESat 2 makes another pass in a closeby track  sea ice conditions may have drastically changed  so waiting for the satellite to cover the entire surface through multiple passes is simply not an option      Our project addresses the challenge of fusing these very different data sources combining grid based imagery from satellites with track based observations from altimetry over a highly dynamic phenomena of sea ice  By using deep learning  we aim to translate one type of data into another  enhancing our ability to map sea ice parameters accurately using both supervised and unsupervised methods  and striving for effective results with minimal training samples  Our goal is to increase the spatial coverage of ICESat 2 measurements by leveraging the information collected by Sentinel 1 SAR  primary  and hopefully Sentinel 2 optical  secondary       Intellectual Merit    This project makes scientific contributions in how satellite data can be integrated and interpreted  By developing algorithms that can fuse grid based SAR  and optical  scences with along the track laser altimetry  we tackle the core scientific challenge of resolving the ambiguities inherent in each data type alone  and expanding the spatial coverage of both  Our deep learning models aim to harness the strengths of each sensing method  providing a more comprehensive understanding of sea ice conditions  The techniques we develop for efficient data fusion with minimal samples have the potential to change data fusion for remote sensing applications in cryospheric sciences and beyond in environmental monitoring      Broader Impacts    The methods and products developed in this project have the potential to change sea ice mapping with significant implications for maritime navigation and climate monitoring  One of the major challenges in mapping sea ice is the lack of precise information related to sea ice thickness for training algorithms  Multi year ice  which has survived a summer melt event  can be tens of meters in thickness  with most of it under water  unobservable from space  can pose significant danger to marine navigation  Multi year ice also modifies the heat transfer between the ocean and atmosphere  among other environmentally significant characteristics  However  mapping it from space using SAR or optical data is a very challenging task  While ICESat 2 altimetry allows for observing sea ice elevation  the limited spatial footprint limits its use in mapping  With efficient fusion  which is the goal of this project  more accurate and timely sea ice charts help ensure safer navigation routes through polar regions  support climate change models with better data  and enable more effective monitoring of environmental changes in vulnerable ecosystems  This project not only enhances our scientific understanding of polar environments  but also demonstrates the potential of artificial intelligence to address global challenges  Publications and reports coming soon " class="project-excerpt-content">
      This project integrates remote sensing and altimetry data to enhance sea ice mapping, with applications and methods generalizable to other application areas, including land remote sensing. We utilize Synthetic Aperture Radar (SAR) from Sentinel-1, which provides high-resolution images capable of penetrating clouds and darkness, and optical satellite imagery from Sentinel-2,...
      <a href="/preview/pr-47/projects/fusion-altimetry.html" class="read-more">Continue reading.</a>
    </p>
  </div>

  
    <div class="project-image-container">
      <a href="/preview/pr-47/projects/fusion-altimetry.html" aria-label="View Fusing Remote Sensing Imagery and Satellite-Based Laser Altimetry (ICESat-2)">
        <img src="https://res.cloudinary.com/dz3zgmhnr/image/upload/v1732734238/50546603-57dd-4b98-b2a7-9ed6364bbf48.png" alt="Fusing Remote Sensing Imagery and Satellite-Based Laser Altimetry (ICESat-2)" class="project-image">
      </a>
    </div>
  
</div>
  </section>


    </main>
    


<footer class="background" style="--image: url('/preview/pr-47/images/background.jpg')" data-dark="true" data-size="wide">

<div style="display: flex; flex-wrap: wrap; justify-content: space-between; align-items: center; max-width: 100%; margin: 0 auto; padding: 0 0px;">
  <div style="flex: 1; text-align: left; min-width: 300px; padding-right: 20px;">
    <a href="/preview/pr-47/contact">Contact Us</a>
  </div>
  <div style="flex: 1; text-align: right; min-width: 300px; white-space: nowrap; padding-top: 0px;">
    © 2026 GeoHAI
      |   Source forked from 
    <a href="https://github.com/greenelab/lab-website-template" target="_blank">LWT</a>
  </div>
</div>

  <!-- removing the links for now. -->
  <!-- <div>
    
      
      
      



  <div class="button-wrapper">
    <a
      class="button"
      href="/preview/pr-47"
      
        data-tooltip="Email"
      
      data-style="bare"
      
      aria-label="Email"
    >
      <i class="icon fa-solid fa-envelope"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a
      class="button"
      href="https://orcid.org/0000-0002-6498-1763"
      
        data-tooltip="ORCID"
      
      data-style="bare"
      
      aria-label="ORCID"
    >
      <i class="icon fa-brands fa-orcid"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a
      class="button"
      href="https://scholar.google.com/citations?user=Vy2oR2kAAAAJ&sortby=pubdate"
      
        data-tooltip="Google Scholar"
      
      data-style="bare"
      
      aria-label="Google Scholar"
    >
      <i class="icon fa-brands fa-google"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a
      class="button"
      href="https://github.com/geohai"
      
        data-tooltip="GitHub"
      
      data-style="bare"
      
      aria-label="GitHub"
    >
      <i class="icon fa-brands fa-github"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a
      class="button"
      href="https://twitter.com/geohai_lab"
      
        data-tooltip="Twitter"
      
      data-style="bare"
      
      aria-label="Twitter"
    >
      <i class="icon fa-brands fa-twitter"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a
      class="button"
      href="https://youtube.com/geohai_lab"
      
        data-tooltip="YouTube"
      
      data-style="bare"
      
      aria-label="YouTube"
    >
      <i class="icon fa-brands fa-youtube"></i>
      
    </a>
  </div>


    
  </div> -->

  <!-- moved to the contact page -->
  <!-- <div>
    &copy; 2026
    GeoHAI
    &nbsp; | &nbsp; Built with
    <a href="https://github.com/greenelab/lab-website-template">
      Lab Website Template
    </a>
  </div> -->

  <input type="checkbox" class="dark-toggle" data-tooltip="Dark mode" aria-label="toggle dark mode" oninput="onDarkToggleChange(event)">
</footer>

  </body>
</html>
